{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Set Up Your Environment\n",
    "\n",
    "### 1.1 Install Apache Spark (Windows 11 Example).\n",
    "\n",
    "#### **Perquisites**  \n",
    "- Java JDK (8 or later, but don't exceed 17.0) <br>\n",
    "Spark runs on the JVM, so Java is required.\n",
    "\n",
    "- Python 3.x <br>\n",
    "For PySpark support and Jupyter integration.\n",
    "\n",
    "- Environment Variables (JAVA_HOME, SPARK_HOME, etc.) <br>\n",
    "---\n",
    "\n",
    "#### **Install Java JDK**\n",
    "Apache Spark requires Java (version 8 or later is fine). <br>\n",
    "\n",
    "a. Download JDK <br>\n",
    "Go to: https://adoptium.net\n",
    "\n",
    "Select: Temurin 11 (LTS recommended) <br>\n",
    "\n",
    "Download the Windows x64 MSI installer <br>\n",
    "\n",
    "b. Install and Set JAVA_HOME <br>\n",
    "After installing:\n",
    "\n",
    "#### **Open Environment Variables (Win + S - search for Environment Variables)**\n",
    "\n",
    "Under System variables, click New: <br>\n",
    "\n",
    "Name: JAVA_HOME <br>\n",
    "\n",
    "Value: C:\\Program Files\\Eclipse Adoptium\\jdk-11.x.x (or whatever matches your version) <br>\n",
    "\n",
    "Add %JAVA_HOME%\\bin to the Path variable. <br>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Install Apache Spark**\n",
    "a. Download Spark Binary <br>\n",
    "Go to: https://spark.apache.org/downloads.html\n",
    "\n",
    "Choose a version (e.g., Spark 3.5.0) <br>\n",
    "\n",
    "Choose a package type: (Pre-built for Apache Hadoop 3) <br>\n",
    "\n",
    "Download and unzip the folder (e.g., to C:\\spark) <br>\n",
    "\n",
    "b. Set SPARK_HOME and PATH <br>\n",
    "In Environment Variables:\n",
    "\n",
    "Add a new variable: <br>\n",
    "\n",
    "Name: SPARK_HOME <br>\n",
    "\n",
    "Value: C:\\spark <br>\n",
    "\n",
    "Add %SPARK_HOME%\\bin to the Path <br>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Install winutils.exe (for Hadoop compatibility)**\n",
    "Spark on Windows needs a workaround to run without full Hadoop. <br>\n",
    "\n",
    "a. Download winutils.exe <br>\n",
    "Go to: <br>\n",
    "https://github.com/steveloughran/winutils\n",
    "\n",
    "Find your Hadoop version (e.g., hadoop-3.3.1\\bin\\winutils.exe) <br>\n",
    "\n",
    "Place the winutils.exe file in: C:\\hadoop\\bin <br>\n",
    "\n",
    "b. Set HADOOP_HOME <br>\n",
    "In Environment Variables:\n",
    "\n",
    "Name: HADOOP_HOME <br>\n",
    "\n",
    "Value: C:\\hadoop <br>\n",
    "\n",
    "Also add %HADOOP_HOME%\\bin to your Path <br>\n",
    "\n",
    "---\n",
    "#### **Install PySpark in Your Virtual Environment**\n",
    "In VS Code terminal (with virtual environment activated): <br>\n",
    "    - (In VS Code CTRL + ` to open the Terminal Powershell Environment)\n",
    "\n",
    "```bash\n",
    "pip install pyspark\n",
    "```\n",
    "\n",
    "(Optional: for Jupyter support)\n",
    "\n",
    "```bash\n",
    "pip install notebook findspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 Set Up SQLite JDBC Driver\n",
    "\n",
    "#### **Download SQLite JDBC Driver**\n",
    "Go to the official repository: <br>\n",
    "\n",
    "https://github.com/xerial/sqlite-jdbc\n",
    "\n",
    "\n",
    "#### **Direct Download Link for the Latest .jar:**\n",
    "\n",
    "https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/\n",
    "Choose the latest version (e.g., sqlite-jdbc-3.49.1.0.jar). <br>\n",
    "\n",
    "#### **Create a directory to store the driver**\n",
    "Create a folder in your project directory to store the JDBC .jar file: <br>\n",
    "\n",
    "```bash\n",
    "mkdir jdbc_drivers\n",
    "```\n",
    "Then move the downloaded .jar into that folder (e.g., jdbc_drivers/sqlite-jdbc-3.49.1.0.jar). <br>\n",
    "\n",
    "In your Jupyter Notebook or script (spark.ipynb), you’ll reference the JDBC driver when creating the Spark session:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\projects\\smart-store-data-git-hub\\.venv\\lib\\site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Spark Session created successfully!\n"
     ]
    }
   ],
   "source": [
    "%pip install findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SmartSales\") \\\n",
    "    .config(\"spark.jars\", \"jdbc_drivers/sqlite-jdbc-3.49.1.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Verify PySpark Works in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001DFE64D5FD0>\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SmartSales\").getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Verify the SQLite JDBC Driver Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-----------+----------+--------+-----------+-----------+----------------+------------+\n",
      "|transaction_id| sale_date|customer_id|product_id|store_id|campaign_id|sale_amount|discount_percent|payment_type|\n",
      "+--------------+----------+-----------+----------+--------+-----------+-----------+----------------+------------+\n",
      "|           550|2024-01-06|       1008|       102|     404|          0|       39.1|               0|        VISA|\n",
      "|           551|2024-01-06|       1009|       105|     403|          0|      19.78|               2|        VISA|\n",
      "|           552|2024-01-16|       1004|       107|     404|          0|      335.1|               0|          MC|\n",
      "|           553|2024-01-16|       1006|       102|     406|          0|      195.5|               0|        VISA|\n",
      "|           554|2024-01-25|       1005|       102|     405|          0|      117.3|               0|        VISA|\n",
      "+--------------+----------+-----------+----------+--------+-----------+-----------+----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:sqlite:data/dw/smart_sales.db\") \\\n",
    "    .option(\"dbtable\", \"sales\") \\\n",
    "    .option(\"driver\", \"org.sqlite.JDBC\") \\\n",
    "    .load()\n",
    "\n",
    "df_sales.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Connect to Your Data Warehouse\n",
    "\n",
    "### 2.1 Start a Jupyter Notebook\n",
    "\n",
    "See above steps. <br>\n",
    "\n",
    "### 2.2 Connect to SQLite using PySpark\n",
    "\n",
    "Using Spark’s JDBC API to connect to the SQLite database `smart_sales.db`, which exists in the /data/dw/ folder. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001DFE64D5FD0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SmartSales\") \\\n",
    "    .config(\"spark.jars\", \"jdbc_drivers/sqlite-jdbc-3.49.1.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Verify Tables are Loaded\n",
    "\n",
    "#### 2.3.1. Sales\n",
    "Loading a table (sales) from the SQLite database into a Spark DataFrame and displaying it. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-----------+----------+--------+-----------+-----------+----------------+------------+\n",
      "|transaction_id| sale_date|customer_id|product_id|store_id|campaign_id|sale_amount|discount_percent|payment_type|\n",
      "+--------------+----------+-----------+----------+--------+-----------+-----------+----------------+------------+\n",
      "|           550|2024-01-06|       1008|       102|     404|          0|       39.1|               0|        VISA|\n",
      "|           551|2024-01-06|       1009|       105|     403|          0|      19.78|               2|        VISA|\n",
      "|           552|2024-01-16|       1004|       107|     404|          0|      335.1|               0|          MC|\n",
      "|           553|2024-01-16|       1006|       102|     406|          0|      195.5|               0|        VISA|\n",
      "|           554|2024-01-25|       1005|       102|     405|          0|      117.3|               0|        VISA|\n",
      "|           555|2024-01-25|       1001|       101|     401|          0|    2379.36|               1|          MC|\n",
      "|           556|2024-01-29|       1009|       104|     403|          0|      172.4|               0|        DISC|\n",
      "|           557|2024-01-29|       1010|       101|     402|          0|    3172.48|               0|          MC|\n",
      "|           558|2024-02-06|       1002|       102|     402|          0|      312.8|               0|        VISA|\n",
      "|           559|2024-02-06|       1001|       106|     401|          0|     622.86|               0|        VISA|\n",
      "|           560|2024-02-06|       1010|       101|     402|          0|    6344.96|               0|        VISA|\n",
      "|           561|2024-02-06|       1005|       107|     405|          0|     469.14|               1|        VISA|\n",
      "|           562|2024-02-08|       1003|       108|     403|          0|      12.56|               0|          MC|\n",
      "|           563|2024-02-08|       1006|       107|     406|          0|      67.02|               0|        VISA|\n",
      "|           564|2024-02-09|       1009|       107|     403|          0|     469.14|               0|        VISA|\n",
      "|           565|2024-02-09|       1002|       105|     402|          0|     138.46|               0|        VISA|\n",
      "|           566|2024-02-24|       1007|       103|     405|          0|     204.84|               1|        cash|\n",
      "|           567|2024-02-24|       1007|       106|     405|          0|      444.9|               0|        VISA|\n",
      "|           568|2024-02-24|       1011|       107|     401|          0|     603.18|               0|        VISA|\n",
      "|           569|2024-02-24|       1010|       101|     402|          0|     3965.6|               2|        VISA|\n",
      "+--------------+----------+-----------+----------+--------+-----------+-----------+----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:sqlite:C:/Projects/smart-store-data-git-hub/data/dw/smart_sales.db\") \\\n",
    "    .option(\"dbtable\", \"sales\") \\\n",
    "    .option(\"driver\", \"org.sqlite.JDBC\") \\\n",
    "    .load()\n",
    "\n",
    "df_sales.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. Products\n",
    "Loading a table (products) from the SQLite database into a Spark DataFrame and displaying it. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+----------+--------------+----------+\n",
      "|product_id|product_name|   category|unit_price|stock_quantity|  supplier|\n",
      "+----------+------------+-----------+----------+--------------+----------+\n",
      "|       101|      laptop|Electronics|    793.12|            15|     China|\n",
      "|       102|      hoodie|   Clothing|      39.1|           154|     China|\n",
      "|       103|       cable|Electronics|     22.76|           560|      Cuba|\n",
      "|       104|         hat|   Clothing|      43.1|          1520|SouthKorea|\n",
      "|       105|    football|     Sports|     19.78|           106|    Russia|\n",
      "|       106|  controller|Electronics|     88.98|            57|    Russia|\n",
      "|       107|      jacket|   Clothing|     67.02|             0|    Canada|\n",
      "|       108|   protector|Electronics|     12.56|            21|    Mexico|\n",
      "+----------+------------+-----------+----------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_products = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:sqlite:C:/Projects/smart-store-data-git-hub/data/dw/smart_sales.db\") \\\n",
    "    .option(\"dbtable\", \"products\") \\\n",
    "    .option(\"driver\", \"org.sqlite.JDBC\") \\\n",
    "    .load()\n",
    "\n",
    "df_products.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3. Customers\n",
    "Loading a table (customers) from the SQLite database into a Spark DataFrame and displaying it. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+------+----------+--------------+----------------+-----------------+\n",
      "|customer_id|            name|region| join_date|loyalty_points|customer_segment|standard_datetime|\n",
      "+-----------+----------------+------+----------+--------------+----------------+-----------------+\n",
      "|       1001|   William White|  East|11/11/2021|          1000|             Tin|       2021-11-11|\n",
      "|       1002|    Wylie Coyote|  East| 2/14/2023|          1250|             Tin|       2023-02-14|\n",
      "|       1003|       Dan Brown|  West|10/19/2023|          3520|            Gold|       2023-10-19|\n",
      "|       1004|       Chewbacca|  West| 11/9/2022|          9650|         Diamond|       2022-11-09|\n",
      "|       1005|          Dr Who| North| 8/18/2023|          2560|             Tin|       2023-08-18|\n",
      "|       1006|   Tiffany James| South|  6/7/2021|           100|         Diamond|       2021-06-07|\n",
      "|       1007|   Susan Johnson| South| 6/30/2023|         69850|          Silver|       2023-06-30|\n",
      "|       1008|      Tony Stark| North|  5/1/2020|         10000|          Silver|       2020-05-01|\n",
      "|       1009|    Jason Bourne|  West| 12/1/2020|         15000|            Gold|       2020-12-01|\n",
      "|       1010|Hermione Granger|  East| 12/9/2022|         15420|         Uranium|       2022-12-09|\n",
      "|       1011| Hermione Grager|  East| 12/9/2022|          1350|             Tin|       2022-12-09|\n",
      "+-----------+----------------+------+----------+--------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:sqlite:C:/Projects/smart-store-data-git-hub/data/dw/smart_sales.db\") \\\n",
    "    .option(\"dbtable\", \"customers\") \\\n",
    "    .option(\"driver\", \"org.sqlite.JDBC\") \\\n",
    "    .load()\n",
    "\n",
    "df_customers.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Query & Aggregate Data\n",
    "\n",
    "#### 3.1 Write a Spark SQL Query for Total Revenue per Customer\n",
    "\n",
    "Using Spark SQL to query the temporary views (`customers`, `sales`) created earlier. This step performs a join between `sales` and `customers`, then aggregates the total sales amount per customer (`SUM`(`sale_amount`)). <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+\n",
      "|            name|total_spent|\n",
      "+----------------+-----------+\n",
      "|   William White|   23752.52|\n",
      "|Hermione Granger|   22822.54|\n",
      "|   Susan Johnson|    12422.6|\n",
      "|       Chewbacca|   11813.44|\n",
      "|   Tiffany James|   11715.82|\n",
      "| Hermione Grager|    8750.94|\n",
      "|    Wylie Coyote|    7434.44|\n",
      "|          Dr Who|    4064.86|\n",
      "|       Dan Brown|     2427.3|\n",
      "|    Jason Bourne|    1806.34|\n",
      "|      Tony Stark|    1545.54|\n",
      "+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create views for Spark SQL\n",
    "df_sales.createOrReplaceTempView(\"sales\")\n",
    "df_customers.createOrReplaceTempView(\"customers\")\n",
    "df_products.createOrReplaceTempView(\"products\")\n",
    "\n",
    "\n",
    "df_top_customers = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    c.name, \n",
    "    ROUND(SUM(s.sale_amount), 2) AS total_spent\n",
    "FROM sales s\n",
    "JOIN customers c ON s.customer_id = c.customer_id\n",
    "GROUP BY c.name\n",
    "ORDER BY total_spent DESC\n",
    "\"\"\")\n",
    "\n",
    "df_top_customers.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Store the Results in a Pandas DataFrame for Visualization\n",
    "Converting the Spark DataFrame to a Pandas DataFrame so you can use visualization libraries like matplotlib or seaborn later in your notebook. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_top_customers_pd = df_top_customers.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Review the Results\n",
    "Visual inspection of the data in the Jupyter notebook by displaying the DataFrame. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>total_spent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>William White</td>\n",
       "      <td>23752.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hermione Granger</td>\n",
       "      <td>22822.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Susan Johnson</td>\n",
       "      <td>12422.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chewbacca</td>\n",
       "      <td>11813.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tiffany James</td>\n",
       "      <td>11715.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name  total_spent\n",
       "0     William White     23752.52\n",
       "1  Hermione Granger     22822.54\n",
       "2     Susan Johnson     12422.60\n",
       "3         Chewbacca     11813.44\n",
       "4     Tiffany James     11715.82"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top_customers_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Slice, Dice, and Drilldown\n",
    "\n",
    "#### 4.1 Slicing: Filter Sales by Date Range\n",
    "Slicing is a technique in Business Intelligence where a filter is apllied to the dataset based on a specific value or range — in this case, we're filtering sales data to include only transactions on or after January 1, 2023. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-----------+----------+--------+-----------+-----------+----------------+------------+\n",
      "|transaction_id| sale_date|customer_id|product_id|store_id|campaign_id|sale_amount|discount_percent|payment_type|\n",
      "+--------------+----------+-----------+----------+--------+-----------+-----------+----------------+------------+\n",
      "|           550|2024-01-06|       1008|       102|     404|          0|       39.1|               0|        VISA|\n",
      "|           551|2024-01-06|       1009|       105|     403|          0|      19.78|               2|        VISA|\n",
      "|           552|2024-01-16|       1004|       107|     404|          0|      335.1|               0|          MC|\n",
      "|           553|2024-01-16|       1006|       102|     406|          0|      195.5|               0|        VISA|\n",
      "|           554|2024-01-25|       1005|       102|     405|          0|      117.3|               0|        VISA|\n",
      "|           555|2024-01-25|       1001|       101|     401|          0|    2379.36|               1|          MC|\n",
      "|           556|2024-01-29|       1009|       104|     403|          0|      172.4|               0|        DISC|\n",
      "|           557|2024-01-29|       1010|       101|     402|          0|    3172.48|               0|          MC|\n",
      "|           558|2024-02-06|       1002|       102|     402|          0|      312.8|               0|        VISA|\n",
      "|           559|2024-02-06|       1001|       106|     401|          0|     622.86|               0|        VISA|\n",
      "|           560|2024-02-06|       1010|       101|     402|          0|    6344.96|               0|        VISA|\n",
      "|           561|2024-02-06|       1005|       107|     405|          0|     469.14|               1|        VISA|\n",
      "|           562|2024-02-08|       1003|       108|     403|          0|      12.56|               0|          MC|\n",
      "|           563|2024-02-08|       1006|       107|     406|          0|      67.02|               0|        VISA|\n",
      "|           564|2024-02-09|       1009|       107|     403|          0|     469.14|               0|        VISA|\n",
      "|           565|2024-02-09|       1002|       105|     402|          0|     138.46|               0|        VISA|\n",
      "|           566|2024-02-24|       1007|       103|     405|          0|     204.84|               1|        cash|\n",
      "|           567|2024-02-24|       1007|       106|     405|          0|      444.9|               0|        VISA|\n",
      "|           568|2024-02-24|       1011|       107|     401|          0|     603.18|               0|        VISA|\n",
      "|           569|2024-02-24|       1010|       101|     402|          0|     3965.6|               2|        VISA|\n",
      "+--------------+----------+-----------+----------+--------+-----------+-----------+----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df_sales = df_sales.withColumn(\"sale_date\", to_date(\"sale_date\"))\n",
    "df_filtered = df_sales.filter(df_sales.sale_date >= \"2023-01-01\")\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Dicing: Group Sales by Product and Region\n",
    "Dicing is analyzing data from multiple dimensions. By grouping sales by product name and customer region to get a breakdown of performance. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------+---------+---------+\n",
      "|product_name|     East|   North|    South|     West|\n",
      "+------------+---------+--------+---------+---------+\n",
      "|       cable|   227.60|    NULL|   204.84|    22.76|\n",
      "|  controller| 1,245.72|1,690.62| 1,245.72|     NULL|\n",
      "|      laptop|56,311.52|    NULL|19,034.88|13,483.04|\n",
      "|    football|   276.92|  336.26|    39.56|    19.78|\n",
      "|         hat| 1,508.50|  818.90|   991.30|   689.60|\n",
      "|      jacket| 2,144.64|2,412.72| 1,809.54|   938.28|\n",
      "|      hoodie|   430.10|  351.90|   586.50|   742.90|\n",
      "|   protector|   615.44|    NULL|   226.08|   150.72|\n",
      "+------------+---------+--------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, format_number\n",
    "\n",
    "# Join necessary tables\n",
    "df_matrix = df_sales.join(df_products, \"product_id\") \\\n",
    "                    .join(df_customers, \"customer_id\")\n",
    "\n",
    "# Create matrix using pivot\n",
    "df_product_region_matrix = df_matrix.groupBy(\"product_name\") \\\n",
    "    .pivot(\"region\") \\\n",
    "    .agg(format_number(sum(\"sale_amount\"), 2))  # format to 2 decimal places\n",
    "\n",
    "df_product_region_matrix.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Drilldown: Aggregate Sales by Year -> Quarter -> Month\n",
    "Drilldown allows users to navigate from summarized data into more detailed levels. Analyzing sales first by Year, then Quarter, then Month. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label total_sales\n",
      "0      2024   108556.34\n",
      "1        Q1    33172.66\n",
      "2        Q2    34372.02\n",
      "3        Q3    35158.12\n",
      "4        Q4     5853.54\n",
      "5        M1     6431.02\n",
      "6        M2    14591.46\n",
      "7        M3    12150.18\n",
      "8        M4    16945.10\n",
      "9        M5    10982.82\n",
      "10       M6     6444.10\n",
      "11       M7    21038.12\n",
      "12       M8     4627.20\n",
      "13       M9     9492.80\n",
      "14      M10     5853.54\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, quarter, month, sum as _sum\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Extract date parts from sale_date\n",
    "df_sales_with_date = df_sales.withColumn(\"year\", year(\"sale_date\")) \\\n",
    "                             .withColumn(\"quarter\", quarter(\"sale_date\")) \\\n",
    "                             .withColumn(\"month\", month(\"sale_date\"))\n",
    "\n",
    "# Step 2: Group by Year\n",
    "df_year = df_sales_with_date.groupBy(\"year\") \\\n",
    "    .agg(_sum(\"sale_amount\").alias(\"total_sales\")) \\\n",
    "    .orderBy(\"year\") \\\n",
    "    .toPandas()\n",
    "df_year[\"label\"] = df_year[\"year\"].astype(str)\n",
    "df_year[\"level\"] = \"year\"\n",
    "df_year[\"total_sales\"] = df_year[\"total_sales\"].map(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "# Step 3: Group by Year + Quarter\n",
    "df_quarter = df_sales_with_date.groupBy(\"year\", \"quarter\") \\\n",
    "    .agg(_sum(\"sale_amount\").alias(\"total_sales\")) \\\n",
    "    .orderBy(\"year\", \"quarter\") \\\n",
    "    .toPandas()\n",
    "df_quarter[\"label\"] = \"  Q\" + df_quarter[\"quarter\"].astype(str)\n",
    "df_quarter[\"level\"] = \"quarter\"\n",
    "df_quarter[\"total_sales\"] = df_quarter[\"total_sales\"].map(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "# Step 4: Group by Year + Quarter + Month\n",
    "df_month = df_sales_with_date.groupBy(\"year\", \"quarter\", \"month\") \\\n",
    "    .agg(_sum(\"sale_amount\").alias(\"total_sales\")) \\\n",
    "    .orderBy(\"year\", \"quarter\", \"month\") \\\n",
    "    .toPandas()\n",
    "df_month[\"label\"] = \"    M\" + df_month[\"month\"].astype(str)\n",
    "df_month[\"level\"] = \"month\"\n",
    "df_month[\"total_sales\"] = df_month[\"total_sales\"].map(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "# Step 5: Combine all levels into one DataFrame\n",
    "combined = pd.concat([\n",
    "    df_year[[\"year\", \"label\", \"total_sales\", \"level\"]],\n",
    "    df_quarter[[\"year\", \"label\", \"total_sales\", \"level\"]],\n",
    "    df_month[[\"year\", \"label\", \"total_sales\", \"level\"]]\n",
    "]).sort_values(by=[\"year\", \"level\"], key=lambda col: col.map({\"year\": 0, \"quarter\": 1, \"month\": 2}))\n",
    "\n",
    "combined.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display in notebook\n",
    "print(combined[[\"label\", \"total_sales\"]])\n",
    "\n",
    "# Save to text file\n",
    "output_path = \"C:/Projects/smart-store-data-git-hub/data/processed/2024_Drilldown.txt\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(combined[[\"label\", \"total_sales\"]].to_string(index=False, justify=\"left\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
